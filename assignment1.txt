Assignment #1
Text Mining Assignment
Text Classification with Machine Learning
Assignment Overview: In this assignment, students will perform text preprocessing, feature
extraction, and build machine learning models for a binary text classification task: “Differentiate
Human-Written and AI-Written Text”
. Students will develop classifiers to predict two labels using
machine learning algorithms (either Human-written text: 0, AI-written text: 1).
Classification Task: Develop a classifier to accurately distinguish between text that is human-
written and text generated by AI. This is a binary classification task where the labels are defined
as:
▪ Human-written text: 0
▪ AI-written text: 1
Dataset Description: Students will be provided with two files:
1. Training Data: AI_vs_human_train_dataset.xlsx. This file has two labels:
a. Human-written text: 0
b. AI-written text: 1
2. Test Data: Final_test_data.csv. This file contains text essays without labels. Students
must predict labels for each essay.
Assignment Tasks:
Task 1: Data Exploration and Understanding (10 points)
1. Load the training dataset (AI_vs_human_train_dataset.xlsx) and examine its structure
2. Display basic statistics (number of samples, classes, class distribution)
3. Show sample texts from each category
4. Visualize the class distribution using appropriate plots
Task 2: Text Preprocessing (30 points)
2.1 Stop Words Removal (10 points): Remove stop words using any of the following libraries:
• NLTK: from nltk.corpus import stopwords
• spaCy: nlp.Defaults.stop_words
Deliverables:
• Show before and after examples of stop word removal
• Compare the vocabulary size before and after stop word removal
2.2 Stemming Techniques (10 points): Students can use any library (e.g., NLTK, spaCy), and
then apply stemming using one or more of the following algorithms:
• Porter Stemmer (for example, NLTK: PorterStemmer())
• Snowball Stemmer (for example, NLTK: SnowballStemmer('english'))
• Lancaster Stemmer (for example, NLTK: LancasterStemmer())
2.3 Lemmatization Techniques (10 points): Students can use any library (e.g., NLTK, spaCy),
and then apply lemmatization using:
• NLTK WordNet Lemmatizer (for example, WordNetLemmatizer())
• spaCy Lemmatizer (built-in with spaCy models)
Deliverables for 2.2 & 2.3:
• Show examples of original words, stemmed versions, and lemmatized versions
• Compare the effects of different stemming/lemmatization techniques
Task 3: Feature Extraction (20 points): Convert preprocessed text into numerical features
using:
1. TF-IDF Vectorization (TfidfVectorizer from scikit-learn)
2. Bag of Words (CountVectorizer from scikit-learn)
Requirements:
• Experiment with different n-gram ranges (unigrams, bigrams, trigrams)
• Set appropriate parameters (max_features, min_df, max_df)
Task 4: Machine Learning Model Development (30 points)
4.1 Support Vector Machine (SVM) (15 points)
• Implement Support Vector Machine (SVM): https://scikit-
learn.org/stable/modules/svm.html
• SVM using sklearn.svm.SVC: https://scikit-learn.org/stable/modules/svm.html
• Experiment with different kernels: kernel='linear', kernel='rbf'
4.2 Decision Tree (15 points)
• Implement Decision Tree (DT): https://scikit-learn.org/stable/modules/tree.html
• DT using sklearn.tree.DecisionTreeClassifier
• Experiment with different parameters:
o criterion: 'gini' vs 'entropy'
o max_depth: different depth limits
o min_samples_split and min_samples_leaf
Task 5: Model Evaluation (10 points)
5.1 Performance Metrics: Calculate and report the following metrics for both models: Accuracy,
Precision, Recall, and F1-score (macro and weighted averages), Confusion Matrix
5.2 Model Comparison: Create a comparison table of SVM vs Decision Tree performance, and
Plot ROC curves or precision-recall curves and discuss which model performs better and why
5.3 Error Analysis: Analyze misclassified examples, and identify patterns in classification errors
Technical Requirements: Use the Libraries below
1. Text Processing:
▪ import pandas as pd
▪ import numpy as np
▪ import matplotlib.pyplot as plt
▪ import seaborn as sns
2. Machine learning
▪ from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
▪ from sklearn.svm import SVC
▪ from sklearn.tree import DecisionTreeClassifier
▪ from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,
f1_score
3. Visualization
▪ from sklearn.tree import plot_tree
▪ import seaborn as sns
Code Structure: Organize your code into the following sections:
1. Data loading and exploration
2. Text preprocessing functions
3. Feature extraction
4. Model training and hyperparameter tuning
5. Evaluation and visualization
6. Results discussion
Submission Guidelines: Submit all files in a single ZIP folder, and use the following name format
(e.g., Maaz_Amjad_assignment 2_R12345678.zip), and put the following files in it, and then
upload them to Canvas.
1. Jupyter Code File Code Submission: Save your code in an .ipynb or .py format. Rename
your code in this format (e.g., Maaz_Amjad_assignment 1.ipynb (or .py)
Requirements:
▪ Well-documented code with markdown explanations
▪ Clear section headers and step-by-step implementation
▪ All visualizations and results tables included
2. Prediction Results: Predicted labels for test data with columns: essay_id, predicted_label,
create a CSV file with the predicted labels and save it using the following format
(e.g., Maaz_Amjad_test_R12345678.csv).
Evaluation Criteria
Component Points Evaluation Criteria
Data Exploration 10 Complete dataset analysis, meaningful visualizations, statistical
insights
Preprocessing 30 Correct implementation of all techniques, comparative analysis,
code quality
Feature Extraction 20 Proper TF-IDF and BoW usage, parameter optimization,
dimensionality analysis
Model
Development
30 SVM and Decision Tree implementation, hyperparameter
tuning, visualization
Evaluation 10 Comprehensive metrics, comparison analysis, error analysis,
final predictions
Total 100